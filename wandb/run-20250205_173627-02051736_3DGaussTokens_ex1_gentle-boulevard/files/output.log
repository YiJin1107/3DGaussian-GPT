Max Steps | First cycle: 146000
Sanity Checking: 0it [00:00, ?it/s]
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..
`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..
[rank: 0] Global seed set to 1866
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 1 processes
----------------------------------------------------------------------------------------------------
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name        | Type          | Params
----------------------------------------------
0 | encoder     | Ptv3Encoder   | 46.4 M
1 | pre_quant   | Linear        | 8.3 K
2 | vq          | ResidualVQ    | 0
3 | pos_encoder | Sequential    | 4.3 K
4 | post_quant  | Linear        | 66.0 K
5 | decoder     | ResNetDecoder | 17.8 M
----------------------------------------------
64.3 M    Trainable params
0         Non-trainable params
64.3 M    Total params










































































Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [50:30<00:00, 41.51s/it, v_num=vard, train/loss=1.010, train/vq_loss=0.224]

















































































Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [33:57<00:00, 27.92s/it, v_num=vard, train/loss=0.401, train/vq_loss=0.103]













































Epoch 2:  45%|â–ˆâ–Š  | 33/73 [16:08<19:33, 29.34s/it, v_num=vard, train/loss=0.276, train/vq_loss=0.0857]
[34m[1mwandb[39m[22m: 429 encountered (Filestream rate limit exceeded, retrying in 2.435103947505007 seconds), retrying request







































Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [33:26<00:00, 27.48s/it, v_num=vard, train/loss=0.186, train/vq_loss=0.0677]




















































































Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [29:48<00:00, 24.50s/it, v_num=vard, train/loss=0.180, train/vq_loss=0.0536]


















































































Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [27:08<00:00, 22.31s/it, v_num=vard, train/loss=0.103, train/vq_loss=0.0421]




















































































Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [27:36<00:00, 22.70s/it, v_num=vard, train/loss=0.0766, train/vq_loss=0.0302]





















































































Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [26:29<00:00, 21.78s/it, v_num=vard, train/loss=0.132, train/vq_loss=0.0348]



















































































Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [27:05<00:00, 22.27s/it, v_num=vard, train/loss=0.0904, train/vq_loss=0.0266]





















































































Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [27:30<00:00, 22.61s/it, v_num=vard, train/loss=0.0827, train/vq_loss=0.0225]




















































































Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [26:57<00:00, 22.15s/it, v_num=vard, train/loss=0.0759, train/vq_loss=0.0215]




















































































Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [27:50<00:00, 22.89s/it, v_num=vard, train/loss=0.0559, train/vq_loss=0.0215]














